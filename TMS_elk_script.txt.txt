-------------------------#keep Elasticsearch synchronized with a relational database using Logstash and JDBC------------------
Ref:https://www.elastic.co/blog/how-to-keep-elasticsearch-synchronized-with-a-relational-database-using-logstash
Ref:https://www.elastic.co/guide/en/logstash/current/plugins-inputs-jdbc.html #for JDBC plugin options
-#1. The following conditions must be satisfied:

--1.As documents in MySQL are written into Elasticsearch, the "_id" field in Elasticsearch must be set to the "id" field from MySQL. 
This provides a direct mapping between the MySQL record and the Elasticsearch document. If a record is updated in MySQL, then the entire associated document will be 
overwritten in Elasticsearch. Note that overwriting a document in Elasticsearch is just as efficient as an update operation would be, because internally, 
an update would consist of deleting the old document and then indexing an entirely new document.

--2.When a record is inserted or updated in MySQL, that record must have a field that contains the update or insertion time. 
This field is used to allow Logstash to request only documents that have been modified or inserted since the last iteration of its polling loop. 
Each time Logstash polls MySQL, it stores the update or insertion time of the last record that it has read from MySQL. On its next iteration, 
Logstash knows that it only needs to request records with an update or insertion time that is newer than the last record that was received in the previous 
iteration of the polling loop

-#2. MySQL setup

CREATE DATABASE es_db;
USE es_db;
DROP TABLE IF EXISTS es_table;
CREATE TABLE es_table (
  id BIGINT(20) UNSIGNED NOT NULL,
  PRIMARY KEY (id),
  UNIQUE KEY unique_id (id),
  client_name VARCHAR(32) NOT NULL,
  modification_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,
  insertion_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP
);

#id: This is the unique identifier for this record. Notice that “id” is defined as the PRIMARY KEY as well as a UNIQUE KEY. 
This guarantees each “id” only appears once in the current table. This will be translated to “_id” for updating or inserting the document into Elasticsearch.
#modification_time: This field is defined so that any insertion or update of a record in MySQL will cause its value to be set to the time of the modification. 
This modification time allows us to pull out any records that have been modified since the last time Logstash requested documents from MySQL.

-#3 MySQL Operations
To insert a record: INSERT INTO es_table (id, client_name) VALUES (<id>, <client name>);
To update a record: UPDATE es_table SET client_name = <new client name> WHERE id=<id>;
To upsert a record: INSERT INTO es_table (id, client_name) VALUES (<id>, <client name when created>) ON DUPLICATE KEY UPDATE client_name=<client name when updated>;

-#4 Synchronization code
#The following logstash pipeline implements the synchronization code that is described in the previous section:
"
input {
  jdbc {
    jdbc_driver_library => "<path>/mysql-connector-java-8.0.16.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://<MySQL host>:3306/es_db"
    jdbc_user => <my username>
    jdbc_password => <my password>
    jdbc_paging_enabled => true
    tracking_column => "unix_ts_in_secs"
    use_column_value => true
    tracking_column_type => "numeric"
    schedule => "*/5 * * * * *"
    statement => "SELECT *, UNIX_TIMESTAMP(modification_time) AS unix_ts_in_secs FROM es_table WHERE (UNIX_TIMESTAMP(modification_time) > :sql_last_value AND modification_time < NOW()) ORDER BY modification_time ASC"
  }
}
filter {
  mutate {
    copy => { "id" => "[@metadata][_id]"}
    remove_field => ["id", "@version", "unix_ts_in_secs"]
  }
}
output {
  # stdout { codec =>  "rubydebug"}
  elasticsearch {
      index => "rdbms_sync_idx"
      document_id => "%{[@metadata][_id]}"
  }
}
"
--#synchronization code explained

- tracking_column: This field specifies the field “unix_ts_in_secs” (described below) that is used for tracking the last document read by Logstash from MySQL, 
and is stored on disk in .logstash_jdbc_last_run. This value will be used to determine the starting value for documents that Logstash will request in the next 
iteration of its polling loop. The value stored in .logstash_jdbc_last_run can be accessed in the SELECT statement as “:sql_last_value”.
- unix_ts_in_secs: This is a field that is generated by the above SELECT statement, and which contains the “modification_time” as a standard Unix timestamp 
(seconds since the epoch). This field is referenced by the “tracking column” that we just discussed. A Unix timestamp is used for tracking progress rather than a 
normal timestamp, as a normal timestamp may cause errors due to the complexity of correctly converting back and forth between UMT and the local timezone.
- sql_last_value: This is a built-in parameter that contains the starting point for the current iteration of Logstash’s polling loop, and it is referenced in the 
SELECT statement line of the above jdbc input configuration. This is set to the most recent value of “unix_ts_in_secs”, which is read from .logstash_jdbc_last_run. 
This is used as the starting point for documents to be returned by the MySQL query that is executed in Logstash’s polling loop. Including this variable in the query 
guarantees that insertions or updates that have previously been propagated to Elasticsearch will not be re-sent to Elasticsearch.
- schedule: This uses cron syntax to specify how often Logstash should poll MySQL for changes. 
The specification of "*/5 * * * * *" tells Logstash to contact MySQL every 5 seconds.
- filter: In this section we simply copy the value of “id” from the MySQL record into a metadata field called “_id”, which we will later reference in the output to 
ensure that each document is written into Elasticsearch with the correct “_id” value. Using a metadata field ensures that this temporary value does not cause a new 
field to be created. We also remove the “id”, “@version”, and “unix_ts_in_secs” fields from the document, as we do not wish for them to be written to Elasticsearch.
- modification_time < NOW(): To send each document to Elasticsearch exactly once without skipping one of the record.

-#5 Testing the system
- insert records in the table es_table
- see the docs into ES
GET rdbms_sync_idx/_search #Go to Kibana> Dev tools and type the command

---------------# Configure bat file as Windows service ---------------------------
Use the tools NSSM - the Non-Sucking Service Manager
Ref: http://nssm.cc/usage

1# Download the last release nssm 2.24 go to any location, copy and unzip it. Par. Users\you
Ref: http://nssm.cc/usage

2#. Navigate to the unziped pathe and install the service
cd nssm-2.24\nssm-2.24\win64>
nssm install kibana-service

3# In Application tab enter the path of the bat file. This is the only required parameter

4#. Optional: Set the output file.
Create you output file on the computer. ~\tms_elklogs\kibana.log
Go to the nssm IO tabs, browse the output(stdout) to the created logfile.

5#.Then create a service.

6#. To start the service go to open the task Manager (sometimes Run as administrator), go to the services tab > Right click > start
To view the service logs, open Windows Powershell, 
cd <log directory>
> Get-Content .\kibana.log.txt -wait

----------------#Windows tips------------------------------------------------------------------------------------------
-------------#Get list of process and their port and kill
netstat -aon | find /i "listening" #then identified the concerning process
#Then open task Manager, search for the PID and end the process. But make also sure if there is no a kibana instance that runs as a service

----------


